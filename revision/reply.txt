We thank the AE and reviewers for the feedback on our work. We have revised the paper as recommended. The feedback was very brief, focusing on two main topics: uniform assumption and including a discussion of alternatives like transparency. These have been addressed in the revision, with changes marked in red in the text.

A point by point guide to the changes in response to reviewer comments is below.

AE:

- This appears to be a useful new tool and the article is well-written.
However, there needs to be a broader discussion of the underlying
problem, e.g., how important is the assumption of uniformity in
high-dimensional space (and how well do the tuning parameters cope
with deviations from the assumption) and how well does the technique
cope with other high-dimensional features (see the reviewer comments).

REPLY: The transformation we are proposing can be considered as purely geometrical, mapping a high-dimensional volume onto a low-dimensional space. It can be used for any type of data and does NOT assume an underlying distribution (uniform or otherwise). We have expanded the explanation of this in the introduction to help the reader and to avoid confusion. We have also added a short section on data preprocessing which discusses the preparation of a dataset for the application of our new display. The applications included in the paper already show a range of different features, none of which are normal or uniformly distributed, and details the necessary preprocessing for those cases.

- Also, there needs to be more discussion of alternative solutions to
problems of overplotting and data crowding, e.g., slicing the projections
and even just semitransparency.  Seen through this lens, could things
like contour lines or other displays of distributions be contemplated?

REPLY: We have extended the discussion on slicing. We have also added a paragraph on alternative solutions to overplotting, such as density displays and transparency, in the introduction.

Reviewer 1:

This paper proposes a way of visualizing high dimensional data while "breaking the curse of dimensionality".

The idea is to loosen the concentration near the mean and spread them toward the outside. This transformation is done so that if data are uniform inside high-dimensional hypersphere, they also appear uniform in the low-dimensional display. Even though I think the idea is interesting and the method is potentially useful in some cases, overall I find this work not contain enough contribution to the field or have in-depth discussion to be published in JCGS.

Also I have a couple of comments.
- The geometric feature of high dimensional data mentioned in introduction that motivates the proposed idea is no longer a concern in the data in reduced dimensions, e.g., the first five principal component scores
(See Figure 7). How would the authors justify the proposed sage method when the dimension is not high to have the curse of dimensionality?

REPLY: The curse of dimensionality is already noticeable with relatively low number of components. In the paper examples with 10 dimensions show that it is clearly already problematic (see Figures 1 and 3). In addition, Figure 4 shows that even the case of p=4 leads to a notable transformation of the radius. We have added a sentence in the introduction that clarifies this use of the term "high-dimensional" in our work.

- As many multivariate statistics assume normality, isnâ€™t it more natural to keep the normality rather than uniformity? For example, it is known that random low-dimensional projections of high-dimensional data tend to behave normal-like. Related to this point, bottom panels in Figure 2 look somewhat distorted and overly corrected.

REPLY: See response to AE.  Figure 2 does not have a bottom panel, so it not clear which graph is being referred to here. The user has the flexibility to tune the transformation. It is entirely possible to over-correct and if this is apparent in the plot as you suggest, the user could reduce gamma, the scaling parameter. 

Reviewer 2:

You have challenged the curse of dimensionality, which is a feature of high-dimensional data. The spike structure of eigenvalues is also the important feature in high-dimensional data analysis (see; Statistica Sinica, Aoshima and Yata, 2020). This structure is different from the spherical concentration, and is a phenomenon observed (for example, gene data analysis). Therefore, you should mention this point as well.

REPLY: Thank you for pointing us to this work. We couldn't find the exact reference but read through http://www3.stat.sinica.edu.tw/statistica/oldpdf/A28n13.pdf and it appears to be on the topic you have mentioned. It is interesting work. It's not relevant for our paper because it deals with very high-dimensional problems. We have now clarified our use of high-dimensions in the introduction. The transformation that we provide will fix issues that occur with what might be called small high-dimensional problems. The work on very high-dimensional data poses even more complicated problems, which can be addressed in a variety of ways. The radial transformation that is the focus of our paper might still be useful after the other measures have been applied. In our examples, a simple approach to the high high-dimension problem was to first do PCA, and then view the first several PCs using the burning sage tour. The fix for the spike eigenvalue distribution could be used as an alternative to PCA, and the burning sage tour could be used to examine more than just the first two components of the dimension reduction. A new section on "Data pre-processing" has been added to the paper to draw attention on common ways to prepare data for visualization using tours.

