---
title: |
  Burning sage: Reversing the curse of dimensionality in the visualisation of high-dimensional data
type: Article
author:
  - name: Ursula Laa
    affil: a, b
    email: ursula.laa@monash.edu
  - name: Dianne Cook
    affil: b
    email: dicook@monash.edu
  - name: Stuart Lee
    affil: b, c
    email: stuart.lee1@monash.edu
affiliation:
  - num: a
    address: |
      School of Physics and Astronomy, Monash University
  - num: b
    address: |
      Department of Econometrics and Business Statistics, Monash University
  - num: c
    address: |
      Molecular Medicine Division, Walter and Eliza Hall Institute, Parkville, Australia
bibliography: biblio.bib
geometry: margin=2.5cm
abstract: |
  XXX
keywords: |
  data visualisation; grand tour; statistical computing; statistical graphics; multivariate data; dynamic graphics
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{setspace}
output: rticles::tf_article
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(tidyverse)
```

\doublespacing
# Introduction

The notion of a curse of dimensionality was originally introduced in @BellmanRichard1961 to describe the increasing difficulty of optimisation in high dimensions when evaluating the objective function on a grid. More generally, we can think of it as a problem of sampling large parameter spaces. As the number of dimensions increases, the volume of the parameter space grows exponentially, and the density of points decreases. As a result observations will be far from the center of the distribution, on the vertices of a simplex [@doi:10.1111/j.1467-9868.2005.00510.x]. This causes problems in different domains, an important example in statistics is cluster analysis: since the distance between any two points in a distribution becomes approximately constant.

A more detailed discussion of the curse of dimensionality was presented in @Donoho00, which also highlighted the opposite effect, a blessing of dimensionality. Indeed we can sometimes exploit large parameter spaces to our advantage. One example is the use of regularisation methods for variable selection. These methods penalise model complexity in the optimisation, for example ridge regression and Lasso. The penalty results in shrinking (some of) the parameter estimates towards zero. In ridge regression we obtain a global shrinkage, while Lasso can be thought of as variable selection and will shrink some of the estimates to zero.

A different view on the curse of dimensionality emerges for dimension reduction and visualisation methods. When working with low-dimensional linear projections from a high-dimensional parameter space, most projections will show approximately Gaussian distributions of the data, and as dimensionality increases, observations start to pile near the center [@diaconis1984]. In the extreme case of high-dimension low-sample size data this results in "data piling", and we can find directions where all observations are in a single point [@10.2307/27639976,@10.1093/biomet/asp084]. These issues also persist with non-linear dimension reduction techniques. This is often referred to as the "crowding problem", which methods like t-Distributed Stochastic Neighbor Embedding (t-SNE) [@tsne] aim to alleviate.

To understand these seemingly contradictory implications, low density of points near the center in the high-dimensional space, and the piling in low-dimensional projections, we need to consider the projected volume across the low-dimensional projection. We can think of the data being distributed in a hypersphere, i.e. we think of a distribution with a maximum distance from the center. Compared to the common assumption of a distribution in a box or hypercube this has the advantage of being rotation invariant, and more accurately captures commonly observed distributions. We can picture the linearly projected volume starting from a 3D sphere. In a 2D projection, a large fraction of the volume will be projected onto a small region near the center of the resulting circle, and will be even further concentrated when projecting onto 1D. The same intuition holds as we increase the number of original dimensions that are projected down to one or two dimensions for visualisation, with increasing fractions of the volume being projected onto a small area around the center. The piling of observaitions then follows directly by assuming that they are uniformly distributed across the volume.

In this work we address the piling in low-dimensional linear projections, which is in particular a barrier for viewing high-dimensional distributions with tour methods [@As85,@BCAH05]. Tours show interpolated sequences of low-dimensional projections of the data. When exploring a dataset with a tour we look at the distribution from different sides and want to discover features that may be visible from a specific viewing angle. However, these features may easily be hidden if most of the observations pile up near the center, while a large fraction of the plotting canvas will only show sparse outlying points. Here we suggest to redistribute the projected points such that equal volume in an original hypersphere will be mapped onto equal areas on a 2D projection plane. This redistribution is achieved through a non-linear transformation of the projected radius of the observations, and essentially results in zooming in near the center of the distribution, while pushing points towards the outer radius of the projected circle if they are further out. 

The paper is structured as follows: the radial transformation is described in Section \ref{sec:method}, the implementation as a new display function for the tourr package is described in Section \ref{sec:implementation}, and we show applications in Section \ref{sec:application}. We conclude in Section \ref{sec:concl}.



XXX see also discussion in https://mc-stan.org/users/documentation/case-studies/curse-dims.html on the average length of multivariate normal vector for large $p$, relation to "concentration of measures".


XXX maybe in real applications we need to do both? Use some form of dimension reduction that can screen some of the noise, followed by zooming in on the center?

# Radial transformation {#sec:method}

As discussed in the introduction, the aim of the radial transformation is to redistribute the projected points, such that equal volume in the original ($p$-dimensional) space is projected onto equal area in a 2D projection. Note that it is straightforward to generalise this for projections onto $d$-dimensional projections in the same way.

The radial dependence of the projected volume was described in @Laa:2020wkm via a ratio of the total volume of a $p$ dimensional sphere of radius $R$, $V(R, p)$ and its projected volume within a 2D radius $r$, $V_{2D}(r, R, p)$,
\begin{equation}
f_{2D} (r, p, R) = \frac{V_{2D}(r, R, p)}{V(R, p)} = 1 - \left(1-\left(\frac{r}{R}\right)^2\right)^{p/2}.
\end{equation}
This ratio is of particular interest because it gives the 2D radial cumulative distribution function (CDF) of points when assuming a uniform distribution within the $p$ dimensional hypersphere. We can compare the radial CDF $f_{2D} (r, p, R)$ to the radial CDF in the full hypersphere,
\begin{equation}
f_{pD} (r, p, R) = \frac{V(r, p)}{V(R, p)} = \left({\frac{r}{R}}\right)^p.
\end{equation}
We show this comparison for selected values in Fig. \ref{fig:cdf}, demonstrating the opposing behaviour as $p$ increases. While $f_{pD}$ concentrates more and more towards large values of $r/R$, $f_{2D}$ has the opposite behaviour, i.e. the volume projected to two dimensions is concentrated near the center of the disk, since we are integrating the large number of orthogonal directions.

```{r cdf, fig.cap="Comparing the relative volume of a $p$ dimensional hypersphere captured within a radius $r$, either in a 2D projection of the sphere, or in the $p$ dimensional space, for $p=3, 10, 100$. While most of the volume is pushed to large values of $r$ in $p$ dimensions, the projected volume is concentrated near the center as $p$ increases.", fig.height=3, fig.width=6, out.width="60%", fig.align = "center"}

cdf_2 <- function(p){
  function(r){
    1 - (1 - r^2)^(p/2)
  }
}

cdf_p <- function(p){
  function(r){
    r^p
  }
}

ggplot(data = data.frame(r = c(0,1)), mapping = aes(x = r)) +
  stat_function(
        fun = cdf_2(3),
        mapping = aes(color = "ca", linetype = "la")) +
  stat_function(
        fun = cdf_p(3),
        mapping = aes(color = "ca", linetype = "lb")) +
    stat_function(
        fun = cdf_2(10),
        mapping = aes(color = "cb",  linetype = "la")) +
    stat_function(
        fun = cdf_p(10),
        mapping = aes(color = "cb",  linetype = "lb")) +
    stat_function(
        fun = cdf_2(100),
        mapping = aes(color = "cc",  linetype = "la")) +
    stat_function(
        fun = cdf_p(100),
        mapping = aes(color = "cc",  linetype = "lb")) +
  scale_color_manual(name = "p",
                     values = RColorBrewer::brewer.pal(3, "Dark2"),
                     labels = c("3", "10", "100")) +
  scale_linetype_manual(name = "f", values = c(1, 2), labels = c("2D", "pD")) +
  xlab("r/R") + ylab("CDF") + theme_bw()

```

## Rescaling the radius

The high density of points near the center in a projection is often unwanted as it makes it difficult to see patterns in the interesting region, and gives large optical weight to the tails of distribution. We can also think about it in terms of mapping a large part of the total volume to a small region in the projection, making it difficult to resolve the relevant features.

Under the assumption that points are from a uniform distribution in a hypersphere, we can invert the effect using the CDF. This is achieved by transforming $r$ such that the points follow the CDF of $f_{2D}(r, R, 2)$ instead, i.e. starting from the distribution that arises for points that are uniform in the $p$ sphere, we find the transformation of $r$ that redistributes points such that they are uniform in a 2D disk.

In practice we work with the 2D polar coordinates $r, \theta$, where $\theta$ is uniform for this distribution (by the rotation invariance of the sphere) and does not need to be transformed. The radial component $r$ is transformed by first calculating the CDF to get a uniform distribution, and then transformed as the inverse of $f_{2D}(r, R, 2)$ to arrive at the desired radial distribution. Thus we replace $r$ by $r'$ according to
\begin{equation}
r'' = f_{2D}(r, R, p) = 1-\left(1-\left(\frac{r}{R}\right)^2\right)^{p/2}
\end{equation}
\begin{equation}
r' = f^{-1}_{2D}(r'', R, 2) = R \sqrt{r''} = R \sqrt{1-\left(1-\left(\frac{r}{R}\right)^2\right)^{p/2}}.
\label{eq:resc}
\end{equation}

The relation between $r'$ and $r$ depends on the number of dimensions $p$, and is illustrated for selected values in Figure \ref{fig:radii}. We see that the transformation is approximately linear near the center. As $p$ increases it becomes non-linear faster, and e.g. for $p=10$ the points with radius $r>0.5$ will alrady be highly distorted and pushed out towards the last eighth in $r'$.

```{r radii, fig.cap="Relation between $r$ and $r'$ for different values of $p$. The scaling is approximately linear near the center, but leads to distortion at large radii when $p$ is large.", fig.height=3, fig.width=6, out.width="60%", fig.align = "center"}
library(tidyverse)
# define index as function of c
trans_p <- function(p){
  function(r){
    sqrt( 1 - (1-r^2)^(p/2))
  }
}
  
# plot dependence for selected values
ggplot(data = data.frame(x = c(0,1)), mapping = aes(x = x)) +
    stat_function(
        fun = trans_p(2),
        mapping = aes(color = "ca")) +
  stat_function(
        fun = trans_p(4),
        mapping = aes(color = "cb")) +
  stat_function(
        fun = trans_p(6),
        mapping = aes(color = "cc")) +
    stat_function(
        fun = trans_p(10),
        mapping = aes(color = "cd")) +
    stat_function(
        fun = trans_p(20),
        mapping = aes(color = "ce")) +
  scale_color_manual(name = "p",
                     values = RColorBrewer::brewer.pal(5, "Dark2"),
                     labels = c("2 (baseline)", "4", "6", "10", "20")) +
  xlab("r") + ylab("r'") + theme_bw()

```

```{r circles, fig.cap="Rescaled equidistant concentric circles."}
library(ggforce)

lbl <- c("p=4", "p=6", "p=10", "p=20")
names(lbl) <- c("p4", "p6", "p10", "p20")

tibble(x = 0, y = 0, r = seq(0.1, 1, length.out = 10)) %>%
  mutate(p4 = trans_p(4)(r)) %>%
  mutate(p6 = trans_p(6)(r)) %>%
  mutate(p10 = trans_p(10)(r)) %>%
  mutate(p20 = trans_p(20)(r)) %>%
  mutate(r = as.factor(r)) %>%
  pivot_longer(cols = starts_with(("p"))) %>%
  mutate(name = factor(name, levels = c("p4", "p6", "p10", "p20"))) %>%
  ggplot() +
  geom_circle(aes(x0=x, y0=y, r=value, color=r)) +
  coord_fixed() +
  theme_bw() +
  facet_wrap(~name, labeller = labeller(name = lbl))

```

## Parametrised rescaling

Eq. \ref{eq:resc} is fixed for a given value of $p$, but we may wish to tune the rescaling more freely. One way of doing this is to replace the dimensionality $p$ by the tuned $p_{eff} = s p$. When $s<1$ the rescaling will be softer, and $s>1$ results in more aggressive rescaling than suggested by $p$ alone. Note that when $p_{eff} < 2$ we actually invert the behaviour and shift the focus away from the center, in general this is not interesting. This is shown in Fig. \ref{fig:scaled} for $p=10$ and selected values of $s$. XXXX this is basically the same plot as before... is there a better illustration to show here, or maybe I don't need any?

```{r scaled, fig.cap="Relation between $r$ and $r'$ for $p=10$ and different values of the scaling $s$. We can tune $s$ to control how strong the rescaling is, with $s<1$ leading to a weaker effect, and large values of $s$ increase the effect.", fig.height=3, fig.width=6, out.width="60%", fig.align = "center"}
library(tidyverse)
# define index as function of c
trans_s <- function(s, p){
  p <- s * p
  function(r){
    sqrt( 1 - (1-r^2)^(p/2))
  }
}
  
# plot dependence for selected values
ggplot(data = data.frame(x = c(0,1)), mapping = aes(x = x)) +
    stat_function(
        fun = trans_s(0.2, 10),
        mapping = aes(color = "ca")) +
  stat_function(
        fun = trans_s(0.5, 10),
        mapping = aes(color = "cb")) +
  stat_function(
        fun = trans_s(1, 10),
        mapping = aes(color = "cc")) +
    stat_function(
        fun = trans_s(2, 10),
        mapping = aes(color = "cd")) +
    stat_function(
        fun = trans_s(5, 10),
        mapping = aes(color = "ce")) +
  scale_color_manual(name = "s",
                     values = RColorBrewer::brewer.pal(5, "Dark2"),
                     labels = c("0.2", "0.5", "1", "2", "5")) +
  xlab("r") + ylab("r'") + theme_bw()

```


# Implementation {#sec:implementation}

By default we calculate $R$ as the maximum $p$ dimensional distance from the center of the distribution. However, this is not robust and might lead to large values because of outlying points. XXX alternatives: percentile based radius? user input radius?

XXX what happens when we reduce dimensionality with PCA? maybe we should use larger $s$ along some notion of "effective dimensionality"? Omitted componenets mainly noise, but we should still consider them as part of the projected space?

Note that for the purpose of fitting the displayed data onto a pre-defined display (e.g. range $[-1,1]$), we can directly scale $r'$, e.g. by a factor $x/R$ where $x$ sets the range on the display, e.g. $x=0.9$ in what follows. We then calculate the new plotting coordinates $x', y'$ from $r', \theta$.


# Applications {#sec:application}

## Image classification

We can use the new display to look at different distribution of images from Google quickdraw (XXX REFERENCE). These are $28\times28=784$ pixel greyscale data, large dataset publicly available, here use small subset of three types of sketches (banana, cactus, crab) and observations (1000 sketches from each class) and see if we can separate the classes in the high-dimensional parameter space. We first reduce dimensionality using PCA and look at the first five PCs.

XXX is this a good example? can see that the fisheye display is using the space better, but I don't think we learn much more compared to what we see with the standard projected display...

```{r sketches, eval=F}
load("sketches_train.rda")
source("display-fisheye.R")
sk_small <- dplyr::filter(sketches, word %in% c("banana", "cactus", "crab")) %>%
  mutate(word = factor(word, levels = c("banana", "cactus", "crab")))
pal <- RColorBrewer::brewer.pal(3, "Dark2")
col <- pal[as.numeric(as.factor(sk_small$word))]
sk_pca <- prcomp(select(sk_small, -word, -id))
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)
sk_5 <- sk_pca$x[,1:5] %>%
  as_tibble() %>%
  mutate_all(scale2)

set.seed(1006)
bases <- save_history(sk_5, max = 5)
tour_path <- interpolate(bases, 0.1)
d <- dim(tour_path)

render(sk_5, planned_tour(bases), display_fisheye(axes="bottomleft", col=col, s=1), "png", "pngs/sketches1-%02d.png", apf=0.1, frames = d[3], rescale = FALSE)
render(sk_5, planned_tour(bases), display_fisheye(axes="bottomleft", col=col, s=2), "png", "pngs/sketches2-%02d.png", apf=0.1, frames = d[3], rescale = FALSE)
render(sk_5, planned_tour(bases), display_xy(axes="bottomleft", col=col), "png", "pngs/sketches3-%02d.png", apf=0.1, frames = d[3], rescale = FALSE)

```



# Discussion {#sec:concl}

Other transformations on the projected data?

